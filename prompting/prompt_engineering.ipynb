{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompting Workshop with Weights and Biases - [Anish Shah](https://www.linkedin.com/in/anish-shah/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu7OtFu3jekl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e8WkuQrkGvY"
      },
      "source": [
        "Welcome to the Prompting Workshop with Weights and Biases, led by Anish Shah. This workshop is designed to explore the fascinating world of prompt engineering, a crucial aspect of interacting with and leveraging the capabilities of large language models (LLMs). Throughout this session, we'll dive into various techniques for crafting effective prompts that can significantly enhance the performance of LLMs across a wide range of tasks.\n",
        "\n",
        "Whether you're new to AI and machine learning or looking to deepen your understanding of prompt engineering, this workshop will provide you with valuable insights and practical skills. By the end of this session, you'll be equipped to design and implement prompts that effectively communicate your intentions to LLMs, enabling more accurate and relevant responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section of the notebook focuses on setting up the environment and installing the required libraries:\n",
        "\n",
        "- It installs the [weave](https://wandb.github.io/weave/) which is used for tracking llm model operations\n",
        "- It installs the `litellm` which is used to standardize model interaction and also make it easy to swap model providers\n",
        "- If running in Google Colab, it retrieves the API keys for Weights & Biases, OpenAI, and Anthropic from the Colab user data and sets them as environment variables.\n",
        "- If not running in Colab, it loads the `.env` file using `load_dotenv()`, making the API keys available as environment variables. This is useful for managing secrets in a local development environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkukrJ2UjfZt"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "else:\n",
        "    !pip install -q python-dotenv\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXAOCskHbxoV"
      },
      "outputs": [],
      "source": [
        "!pip install weave -U -q\n",
        "!pip install litellm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import weave\n",
        "from litellm import completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model and Prompt Configuration\n",
        "The code snippets define important configuration variables for the prompting workshop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ANTHROPIC_SMART_MODEL_NAME = \"claude-3-opus-20240229\"\n",
        "ANTHROPIC_FAST_MODEL_NAME = \"claude-3-haiku-20240307\"\n",
        "OPENAI_SMART_MODEL_NAME = \"gpt-4\"\n",
        "OPENAI_FAST_MODEL_NAME = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These variables store the names of different language models from Anthropic and OpenAI. The \"SMART\" models (`claude-3-opus` and `gpt-4`) are more capable but slower, while the \"FAST\" models (`claude-3-haiku` and `gpt-3.5-turbo`) are faster but less powerful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AMAN_PROMPT_GUIDE = \"aman_prompt_engineering.md\"\n",
        "LILIAN_PROMPT_GUIDE = \"lilianweng_prompt_engineering.md\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These variables point to two different markdown files containing prompt engineering guides. `AMAN_PROMPT_GUIDE` refers to Aman Chadha's guide, while `LILIAN_PROMPT_GUIDE` refers to Lilian Weng's guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = ANTHROPIC_FAST_MODEL_NAME\n",
        "PROMPT_GUIDE = LILIAN_PROMPT_GUIDE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, the `MODEL_NAME` variable is set to use Anthropic's fast model (`claude-3-haiku`), and the `PROMPT_GUIDE` variable selects Lilian Weng's prompt engineering guide.\n",
        "\n",
        "These configuration variables allow workshop participants to easily switch between different models and prompt guides throughout the workshop by modifying the assigned values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializing Weave\n",
        "\n",
        "This line initializes the Weave library for the \"prompting-workshop\" project. Weave is a toolkit for developing Generative AI applications, providing features like logging, debugging, evaluations, and organization of LLM workflows. \n",
        "\n",
        "Initializing Weave at the start allows you to leverage its capabilities throughout your project, such as decorating Python functions with `@weave.op()` to enable automatic tracing and versioning.\n",
        "\n",
        "By specifying the project name \"prompting-workshop\", you are setting up a dedicated workspace for this workshop within Weave. This helps keep the workshop-related experiments, models, and data organized and separate from other projects.\n",
        "\n",
        "Weave brings structure and best practices to the experimental nature of Generative AI development, making it easier to track, reproduce, and share your work. Initializing it early in the notebook ensures you can take full advantage of its features as you progress through the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzV38KVSby6D"
      },
      "outputs": [],
      "source": [
        "weave.init(\"prompting-workshop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the get_completion function\n",
        "\n",
        "This code defines a function called `get_completion` that is decorated with `@weave.op()`. The `@weave.op()` decorator is provided by Weave and enables automatic tracing and versioning of the function.\n",
        "\n",
        "The `get_completion` function takes several parameters:\n",
        "- `system_message`: The system message to provide context or instructions to the language model.\n",
        "- `messages`: A list of messages representing the conversation history.\n",
        "- `model_name`: The name of the language model to use (defaults to `MODEL_NAME`).\n",
        "- `max_tokens`: The maximum number of tokens to generate in the response (defaults to 4096).\n",
        "- `temperature`: The sampling temperature for controlling the randomness of the generated text (defaults to 0).\n",
        "\n",
        "Inside the function, it calls the `completion` function (from the `litellm` library) with the provided parameters to generate a completion from the language model. The `temperature` parameter is set to 0, which is recommended for evaluations and RAG (Retrieval-Augmented Generation) systems to ensure deterministic results.\n",
        "\n",
        "The generated response is then printed as JSON using `response.json()`, and the JSON response is returned by the function.\n",
        "\n",
        "By using the `@weave.op()` decorator, Weave automatically tracks and versions the inputs and outputs of the `get_completion` function, making it easier to reproduce and analyze the results later in the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfuTRXK0b1vm"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def get_completion(system_message, messages, model_name=MODEL_NAME, max_tokens=4096, temperature=0):\n",
        "    response = completion(\n",
        "        model=model_name,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature , #Good to set this for evals and RAG systems to 0\n",
        "        system=system_message,\n",
        "        messages=messages\n",
        "    )\n",
        "    print(response.json())\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use Case: Building a Prompting Assistant\n",
        "\n",
        "In this section, we explore the practical application of prompt engineering by building a bot that helps users understand prompting techniques and answers questions based on the provided information. This use case demonstrates the power of prompt engineering in creating helpful AI assistants that can make complex topics more accessible and engaging.\n",
        "\n",
        "By leveraging the knowledge contained in a comprehensive guide on prompting techniques, we can develop a bot that provides accurate and relevant answers to user queries. Through careful crafting of system messages and prompt templates, we ensure that the bot's responses are not only informative but also easy to understand, even for beginners.\n",
        "\n",
        "Throughout this use case, workshop participants will learn how to:\n",
        "\n",
        "- Incorporate context to improve the relevance and accuracy of the model's responses\n",
        "- Use system messages to guide the model's behavior and output style\n",
        "- Standardize inputs and outputs for consistent and reusable prompting assistants\n",
        "- Experiment with different configurations to optimize the bot's performance\n",
        "\n",
        "By engaging with this use case, participants will gain hands-on experience in applying prompt engineering techniques to build a practical and helpful AI assistant. They will develop a deeper understanding of how to effectively communicate with language models and tailor their outputs to specific audiences and use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Raw Prompting\n",
        "\n",
        "We start by sending a question to the language model without any additional context, using a basic `prompt_llm` function. This demonstrates the model's limitations when lacking the necessary information to provide relevant answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def prompt_llm(question, system_message=\"\"):\n",
        "    return get_completion(system_message=system_message, messages=[{\"role\": \"user\", \"content\": question}])[\"choices\"][0][\"message\"][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_prompt_response = prompt_llm(\n",
        "    \"What are the latest prompting techniques?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(raw_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model's response to the raw prompt is inadequate because it lacks the necessary context to provide a meaningful answer. Without any background information or specific details about prompting techniques, the model can only generate a generic, high-level response that fails to address the question effectively, many times providing no response at all.\n",
        "\n",
        "This poor performance highlights the importance of providing relevant context when prompting language models. By supplying the model with additional information related to the topic at hand, we can guide it towards generating more accurate, detailed, and useful responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Prompting with Context\n",
        "\n",
        "We can provide the necessary context to the language model by including it directly alongside the question. In this example, we use a comprehensive guide on prompting techniques written by [Lilian Weng](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/). This guide is particularly useful as it condenses many great papers and articles into a single-page resource, covering various prompting techniques.\n",
        "\n",
        "To incorporate the context, we:\n",
        "\n",
        "1. Load the markdown file containing the prompting guide using the `load_markdown_file` function.\n",
        "2. Concatenate the loaded context with the question in the `context_prompt_response` variable.\n",
        "3. Pass the combined context and question to the `prompt_llm` function to generate a response.\n",
        "\n",
        "By providing the model with relevant context, we expect to receive more accurate and informative answers to our questions about prompting techniques.\n",
        "\n",
        "Note: As an alternative, you can also use a more extensive guide by [Aman Chadha](https://aman.ai/primers/ai/prompt-engineering/) for additional context and information on prompt engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_markdown_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        markdown_content = file.read()\n",
        "    return markdown_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = load_markdown_file(PROMPT_GUIDE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: Anthropic has an amazingly large context size and as a result we can luckily just shove the whole document into the prompt in this situation. This can get quite expensive however so it typically makes more sense to use techniques that chunk the document into better sizes or use a RAG based pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_prompt_response = prompt_llm(\n",
        "    context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This response is a significant improvement! We can see that by providing the model with relevant context, it can generate an answer that includes details about various prompting techniques covered in the workshop. The model effectively utilizes the information from the provided guide to address the question more comprehensively.\n",
        "\n",
        "However, there is still room for improvement. The model tends to regurgitate the technical information from the guide without simplifying or explaining the concepts in an easily understandable manner. The response may be too complex or jargon-heavy for beginners or those new to the topic of prompt engineering.\n",
        "\n",
        "To address this issue, we need to guide the model towards providing explanations that are more accessible and beginner-friendly. This is where the next step of conditioning the model's responses with a carefully crafted system prompt comes into play. By instructing the model to break down the technical details and present the information in a more digestible format, we can ensure that the responses are not only informative but also easy to understand for a broader audience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Condition Responses with a System Prompt\n",
        "\n",
        "To ensure that the bot explains the information in a way that is easy to understand, we can provide a system prompt that guides the model to present the content in a beginner-friendly manner. The system prompt acts as a set of instructions for the model, outlining the desired tone, style, and level of complexity for the generated responses.\n",
        "\n",
        "In this step, we create a detailed system message that includes the following guidelines:\n",
        "\n",
        "- **Objective**: Analyze the technical markdown page on prompt engineering and simplify the concepts for easy understanding.\n",
        "- **Personality and Tone**: Adopt a friendly and knowledgeable teacher's role, breaking down complex subjects into digestible pieces.\n",
        "- **Contextual Information**: Assume the user has a basic understanding of AI but may not be familiar with advanced prompting concepts.\n",
        "- **Creativity Constraints and Style Guidance**: Avoid jargon and technical terminology, using metaphors, analogies, and simple examples to convey points.\n",
        "- **External Knowledge**: Draw upon general knowledge of AI, machine learning, and prompt engineering practices, but avoid diving into highly specialized research.\n",
        "- **Rules and Guidelines**: Steer clear of overly complex explanations or examples that might confuse beginners, ensuring examples are realistic and directly applicable.\n",
        "- **Output Verification Standards**: Ensure responses are clear, accurate, and directly responsive to the task, with examples that effectively demonstrate the discussed concepts.\n",
        "- **Benefits of Task**: Highlight how simplifying these technical concepts will help demystify prompt engineering for those new to the subject, fostering a deeper understanding and appreciation of its importance in AI interactions.\n",
        "\n",
        "By incorporating this system prompt, we condition the model to generate responses that are more accessible, engaging, and tailored to the needs of beginners in the field of prompt engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "Objective: You will analyze a highly technical markdown page about prompt engineering. Your task is to simplify the concepts discussed on this page and explain them in an easy-to-understand manner. For each prompting technique mentioned, provide clear, concise examples that illuminate how the same task would be approached differently.\n",
        "Personality and Tone: Adopt the role of a friendly and knowledgeable teacher who excels at breaking down complex subjects into easily digestible pieces. Your explanations should be patient and encouraging, aiming to enlighten without overwhelming. The tone should be casual yet informative, making technical content accessible to a broad audience, including beginners.\n",
        "Contextual Information: Assume the user has a basic understanding of AI but may not be familiar with advanced concepts of prompt engineering. Wherever possible, relate technical details to everyday scenarios or familiar contexts to enhance comprehension.\n",
        "Creativity Constraints and Style Guidance: Your explanations should avoid jargon and technical terminology without sacrificing accuracy. Use metaphors, analogies, and simple examples to convey your points. Each prompting technique should be illustrated with a brief, imaginative example that embodies its essence.\n",
        "External Knowledge: Feel free to draw upon general knowledge of AI, machine learning, and prompt engineering practices. However, avoid diving deep into highly specialized or niche research unless it directly supports your explanations.\n",
        "Rules and Guidelines: Steer clear of overly complex explanations or examples that might confuse someone new to the topic. Ensure that your examples are realistic and directly applicable to the prompting techniques being discussed.\n",
        "Output Verification Standards: Your responses should be clear, accurate, and directly responsive to the task. Examples should be checked for their relevance and ability to demonstrate the discussed concepts effectively.\n",
        "Benefits of Task: By simplifying these technical concepts, you will help demystify prompt engineering for those new to the subject, fostering a deeper understanding and appreciation of its importance in AI interactions. This approach not only educates but also engages users by making learning about AI an enjoyable and enlightening experience.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_and_context_prompt_response = prompt_llm(\n",
        "    system_message=system_message,\n",
        "    question=context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(system_and_context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! Now we're able to get a response that is easy to understand and provides a lot of context. The model has successfully broken down the technical concepts into beginner-friendly explanations, using simple language, analogies, and examples to convey the key points. This approach makes the information more accessible and engaging for those new to prompt engineering, fostering a deeper understanding of the subject matter.\n",
        "\n",
        "The next step is to standardize the inputs and outputs in a way that allows us to ask different questions and pass different context in the future. By creating a consistent structure for our prompts and responses, we can streamline the development of LLM applications and make it easier to experiment with various configurations. This standardization will enable us to quickly iterate on our prompts, test different contexts, and fine-tune our models to achieve the best possible results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: System Prompts - Inputs\n",
        "\n",
        "To make it easier to experiment with different parameters and retrieve our best models, we can wrap our prompting logic in a Weave Model. Weave Models provide a structured way to define the inputs, outputs, and behavior of our prompting system, making it more modular and reusable.\n",
        "\n",
        "We define a `PromptingModel` class that inherits from the `Model` class provided by Weave. This class encapsulates the following attributes:\n",
        "\n",
        "- `system_message`: The system message that guides the model's behavior and output style.\n",
        "- `context`: The context or background information to be provided to the model.\n",
        "- `prompt_template`: A template string that defines how the context and question should be formatted in the prompt.\n",
        "- `model_name`: The name of the language model to be used.\n",
        "- `temperature`: The sampling temperature for controlling the randomness of the generated text.\n",
        "\n",
        "The `PromptingModel` class also includes methods for generating prompts and making predictions:\n",
        "\n",
        "- `get_prompt`: This method takes a question as input and returns a formatted prompt based on the `prompt_template`, `context`, and `question`.\n",
        "- `predict`: This method takes a question as input, generates a prompt using `get_prompt`, and sends it to the language model to obtain a response.\n",
        "- `render`: This method takes a question as input, calls predict to generate a response, and renders the response as markdown for better viewing.\n",
        "\n",
        "By encapsulating the prompting logic in a Weave Model, we can easily instantiate and configure different versions of our prompting system, experiment with various parameters, and retrieve the best-performing models. This modular approach simplifies the development process and allows for greater flexibility in fine-tuning our prompts and models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from weave import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptingModel(Model):\n",
        "\n",
        "    system_message: str = \"\"\n",
        "    context: str = \"\"\n",
        "    prompt_template: str = \"{context}\\n{question}\"\n",
        "\n",
        "    model_name: str = MODEL_NAME\n",
        "    max_tokens: int = 4096\n",
        "    temperature: float = 0.0\n",
        "    \n",
        "\n",
        "    def __init__(self, system_message, context, prompt_template=None, model_name=None, max_tokens=None, temperature=None):\n",
        "        super().__init__()\n",
        "        self.system_message = system_message\n",
        "        self.context = context\n",
        "        if prompt_template:\n",
        "            self.prompt_template = prompt_template \n",
        "        if model_name:\n",
        "            self.model_name = model_name\n",
        "        if max_tokens:\n",
        "            self.max_tokens = max_tokens\n",
        "        if temperature:\n",
        "            self.temperature = temperature\n",
        "\n",
        "    # f-strings make for creating great prompt templates\n",
        "    @weave.op()\n",
        "    def get_prompt(self, question):\n",
        "        return [{\"role\": \"user\",\n",
        "                 \"content\": self.prompt_template.format(context=self.context, question=question)\n",
        "                }]\n",
        "\n",
        "    # we change the above prompt_llm to use the get_prompt method\n",
        "    # @weave.op()\n",
        "    # def predict(self, question):\n",
        "    #     response = get_completion(\n",
        "    #         system_message=self.system_message,\n",
        "    #         messages=self.get_prompt(question),\n",
        "    #         model_name=self.model_name,\n",
        "    #         max_tokens=self.max_tokens,\n",
        "    #         temperature=self.temperature\n",
        "    #     )\n",
        "    #     return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question):\n",
        "        response = completion(\n",
        "            model=self.model_name,\n",
        "            max_tokens=self.max_tokens,\n",
        "            temperature=self.temperature , #Good to set this for evals and RAG systems to 0\n",
        "            system=self.system_message,\n",
        "            messages=self.get_prompt(question)\n",
        "        )\n",
        "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    \n",
        "    # we render the response as markdown for better viewing\n",
        "    def render(self, question):\n",
        "        response = self.predict(question)\n",
        "        return Markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note we're assuming we're only using context and question in our prompt template\n",
        "prompt_template = \"{context}\\n{question}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_app = PromptingModel(\n",
        "    system_message=system_message, \n",
        "    context=context, \n",
        "    prompt_template=prompt_template,\n",
        "    model_name=MODEL_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_app.schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_app.get_prompt(\"Explain the latest prompting techniques and provide an example of each\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "I am building a chatbot for analyzing workshop attendee satisfaction. \n",
        "What are some good examples of Few Shot prompts to put into another prompt?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_app.render(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can easily and consistently swap system messages, context, and questions to get the best results. By using the `PromptingModel` class, we have a standardized way to experiment with different configurations and quickly iterate on our prompts. This allows us to test various combinations of system messages, context, and questions to find the most effective prompts for our specific use case.\n",
        "\n",
        "However, as we can see from the example, the current system prompt doesn't work as well with the newly asked question. This highlights the importance of tailoring the system prompt to the specific task at hand. Additionally, we may want to enforce more consistency in the format of our model's outputs. While using third-party packages like Instructor is beyond the scope of this workshop, we can achieve similar results by using proper tags in our prompt. By including specific tags or formatting instructions in the prompt, we can guide the model to respond in a way that is more consistent and easier to parse on our end\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: System Prompts - Outputs\n",
        "\n",
        "In this step, we focus on improving the consistency and structure of our model's outputs by modifying the prompt template. By including specific tags and formatting instructions in the prompt, we can guide the model to respond in a way that is easier to parse and process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "Objective: You will analyze a highly technical markdown page about prompt engineering. Your task is to simplify the concepts discussed on this page and explain them in an easy-to-understand manner. For each prompting technique mentioned, provide clear, concise examples that illuminate how the same task would be approached differently.\n",
        "Personality and Tone: Adopt the role of a friendly and knowledgeable teacher who excels at breaking down complex subjects into easily digestible pieces. Your explanations should be patient and encouraging, aiming to enlighten without overwhelming. The tone should be casual yet informative, making technical content accessible to a broad audience, including beginners.\n",
        "Contextual Information: Assume the user has a basic understanding of AI but may not be familiar with advanced concepts of prompt engineering. Wherever possible, relate technical details to everyday scenarios or familiar contexts to enhance comprehension.\n",
        "Creativity Constraints and Style Guidance: Your explanations should avoid jargon and technical terminology without sacrificing accuracy. Use metaphors, analogies, and simple examples to convey your points. Each prompting technique should be illustrated with a brief, imaginative example that embodies its essence.\n",
        "External Knowledge: Feel free to draw upon general knowledge of AI, machine learning, and prompt engineering practices. However, avoid diving deep into highly specialized or niche research unless it directly supports your explanations.\n",
        "Rules and Guidelines: Steer clear of overly complex explanations or examples that might confuse someone new to the topic. Ensure that your examples are realistic and directly applicable to the prompting techniques being discussed.\n",
        "Output Verification Standards: Your responses should be clear, accurate, and directly responsive to the task. Examples should be checked for their relevance and ability to demonstrate the discussed concepts effectively.\n",
        "Benefits of Task: By simplifying these technical concepts, you will help demystify prompt engineering for those new to the subject, fostering a deeper understanding and appreciation of its importance in AI interactions. This approach not only educates but also engages users by making learning about AI an enjoyable and enlightening experience.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "<context>{context}</context>\n",
        "<question>{question}</question>\\n\n",
        "\n",
        "You must respond within an <answer></answer> markdown tag.\n",
        "Inside of the <answer> markdown tag, you must provide a format of\n",
        "<answer>\n",
        "    <explanation> EXPLANATION </explanation>\n",
        "    <example> EXAMPLE </example>\n",
        "</answer>\n",
        "\n",
        "Fill in the rest of the tag given:\n",
        "<answer>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_app = PromptingModel(\n",
        "    system_message=system_message, \n",
        "    context=context, \n",
        "    prompt_template=prompt_template,\n",
        "    # model_name=ANTHROPIC_SMART_MODEL_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "I am building a chatbot for analyzing workshop attendee satisfaction. \n",
        "What are some good examples of Few Shot prompts to put into another prompt?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_app.render(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Prompting Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chain of Thought"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### CoT + Few-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self-Consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Self-Consistency + CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
