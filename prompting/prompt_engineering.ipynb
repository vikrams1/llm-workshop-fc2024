{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompting Workshop with Weights and Biases - [Anish Shah](https://www.linkedin.com/in/anish-shah/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu7OtFu3jekl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e8WkuQrkGvY"
      },
      "source": [
        "Ensure to have the appropriate API keys for the models you want to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkukrJ2UjfZt"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "else:\n",
        "    !pip install -q python-dotenv\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXAOCskHbxoV"
      },
      "outputs": [],
      "source": [
        "!pip install weave -U -q\n",
        "!pip install litellm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG865m6mkNz3"
      },
      "source": [
        "Begin weave tracking experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import weave\n",
        "from litellm import completion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ANTHROPIC_SMART_MODEL_NAME = \"claude-3-opus-20240229\"\n",
        "ANTHROPIC_FAST_MODEL_NAME = \"claude-3-haiku-20240307\"\n",
        "OPENAI_SMART_MODEL_NAME = \"gpt-4\"\n",
        "OPENAI_FAST_MODEL_NAME = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = ANTHROPIC_FAST_MODEL_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzV38KVSby6D"
      },
      "outputs": [],
      "source": [
        "weave.init(\"prompting-workshop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfuTRXK0b1vm"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def get_completion(system_message, messages, model_name=MODEL_NAME, max_tokens=4096, temperature=0):\n",
        "    response = completion(\n",
        "        model=model_name,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature , #Good to set this for evals and RAG systems to 0\n",
        "        system=system_message,\n",
        "        messages=messages\n",
        "    )\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use case: Building a bot to help us to understand all the prompting information and answer the questions based on the information provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1: Raw prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def prompt_llm(question, system_message=\"\"):\n",
        "    return get_completion(system_message=system_message, messages=[{\"role\": \"user\", \"content\": question}])[\"choices\"][0][\"message\"][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_prompt_response = prompt_llm(\n",
        "    \"What are the latest prompting techniques?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(raw_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanbaly the model is not properly aware of the context of the question. It is just trying to answer the question based on the information provided in the prompt, which in this case is nothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2. Prompting with context\n",
        "\n",
        "We can throw the context directly alongside the question. [I will use this website by Aman Chadha](https://aman.ai/primers/ai/prompt-engineering/) who very usefully condensed many great papers and articles into a comprehensive single page guide of different prompting techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_markdown_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        markdown_content = file.read()\n",
        "    return markdown_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = load_markdown_file('prompt_engineering.md')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: Anthropic has an amazingly large context size and as a result we can luckily just shove the whole document into the prompt in this situation. This can get quite expensive however so it typically makes more sense to use techniques that chunk the document into better sizes or use a RAG based pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_prompt_response = prompt_llm(\n",
        "    context + \"\\n\\nWhat are the latest prompting techniques?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This response is a lot better! We can see that the model is able to answer the question with a lot more context and actually responds with details that explain various techniques I'll be talking about in class. The problem though is that this is regurgitating the information from the website and the technical details are not being explained in a way that is easy to understand. This is where the next step comes in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 3. Condition Responses with a System Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to ensure we force our bot to explain the information in a way that is easy to understand. We can do this by providing a system prompt that forces the model to explain the information in a way that is best for us to understand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "Objective: You will analyze a highly technical markdown page about prompt engineering. Your task is to simplify the concepts discussed on this page and explain them in an easy-to-understand manner. For each prompting technique mentioned, provide clear, concise examples that illuminate how the same task would be approached differently.\n",
        "\n",
        "Personality and Tone: Adopt the role of a friendly and knowledgeable teacher who excels at breaking down complex subjects into easily digestible pieces. Your explanations should be patient and encouraging, aiming to enlighten without overwhelming. The tone should be casual yet informative, making technical content accessible to a broad audience, including beginners.\n",
        "\n",
        "Contextual Information: Assume the user has a basic understanding of AI but may not be familiar with advanced concepts of prompt engineering. Wherever possible, relate technical details to everyday scenarios or familiar contexts to enhance comprehension.\n",
        "\n",
        "Creativity Constraints and Style Guidance: Your explanations should avoid jargon and technical terminology without sacrificing accuracy. Use metaphors, analogies, and simple examples to convey your points. Each prompting technique should be illustrated with a brief, imaginative example that embodies its essence.\n",
        "\n",
        "External Knowledge: Feel free to draw upon general knowledge of AI, machine learning, and prompt engineering practices. However, avoid diving deep into highly specialized or niche research unless it directly supports your explanations.\n",
        "\n",
        "Rules and Guidelines: Steer clear of overly complex explanations or examples that might confuse someone new to the topic. Ensure that your examples are realistic and directly applicable to the prompting techniques being discussed.\n",
        "\n",
        "Output Verification Standards: Your responses should be clear, accurate, and directly responsive to the task. Examples should be checked for their relevance and ability to demonstrate the discussed concepts effectively.\n",
        "\n",
        "Benefits of Task: By simplifying these technical concepts, you will help demystify prompt engineering for those new to the subject, fostering a deeper understanding and appreciation of its importance in AI interactions. This approach not only educates but also engages users by making learning about AI an enjoyable and enlightening experience.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_and_context_prompt_response = prompt_llm(\n",
        "    system_message=system_message,\n",
        "    question=context + \"\\n\\nWhat are the latest prompting techniques?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(system_and_context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! Now the we're able to get a response that is easy to understand and provides a lot of context. Now we can start to standardize the inputs and outputs in such a way that we can ask different questions and even pass different context in the future, making it easy for LLM application development."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 4: System Prompts - Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from weave import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptingModel(Model):\n",
        "\n",
        "    system_message: str = \"\"\n",
        "    context: str = \"\"\n",
        "    prompt_template: str = \"{context}\\n{question}\"\n",
        "\n",
        "    def __init__(self, system_message, context, prompt_template=None):\n",
        "        super().__init__()\n",
        "        self.system_message = system_message\n",
        "        self.context = context\n",
        "        if prompt_template:\n",
        "            self.prompt_template = prompt_template \n",
        "\n",
        "    # f-strings make for creating great prompt templates\n",
        "    @weave.op()\n",
        "    def get_prompt(self, question):\n",
        "        return [{\"role\": \"user\",\n",
        "                 \"content\": self.prompt_template.format(context=self.context, question=question)\n",
        "                }]\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question):\n",
        "        response = get_completion(system_message=self.system_message, messages=self.get_prompt(question))\n",
        "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    \n",
        "    def render(self, question):\n",
        "        response = self.predict(question)\n",
        "        return Markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_app = PromptingModel(system_message, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_app.get_prompt(\"What are the latest prompting techniques?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "I am building a chatbot for analyzing workshop attendee satisfaction. \n",
        "What are some good examples of Few Shot prompts to put into another prompt?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_app.render(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
